{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_ug_cbow = KeyedVectors.load('w2v_model_ug_cbow.word2vec')\n",
    "model_ug_sg = KeyedVectors.load('w2v_model_ug_sg.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] = np.append(model_ug_cbow.wv[w], model_ug_sg.wv[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/.local/lib/python3.5/site-packages/numpy/lib/arraysetops.py:395: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1596036</th>\n",
       "      <td>just woke up having no school is the best feel...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596037</th>\n",
       "      <td>thewdb com very cool to hear old walt interviews</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596038</th>\n",
       "      <td>are you ready for your mojo makeover ask me fo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596039</th>\n",
       "      <td>happy th birthday to my boo of alll time tupac...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596040</th>\n",
       "      <td>happy charitytuesday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  target\n",
       "1596036  just woke up having no school is the best feel...       4\n",
       "1596037   thewdb com very cool to hear old walt interviews       4\n",
       "1596038  are you ready for your mojo makeover ask me fo...       4\n",
       "1596039  happy th birthday to my boo of alll time tupac...       4\n",
       "1596040                               happy charitytuesday       4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = 'df.csv'\n",
    "df = pd.read_csv(csv, index_col=0)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.target == 4, 'target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1596036</th>\n",
       "      <td>just woke up having no school is the best feel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596037</th>\n",
       "      <td>thewdb com very cool to hear old walt interviews</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596038</th>\n",
       "      <td>are you ready for your mojo makeover ask me fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596039</th>\n",
       "      <td>happy th birthday to my boo of alll time tupac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596040</th>\n",
       "      <td>happy charitytuesday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  target\n",
       "1596036  just woke up having no school is the best feel...       1\n",
       "1596037   thewdb com very cool to hear old walt interviews       1\n",
       "1596038  are you ready for your mojo makeover ask me fo...       1\n",
       "1596039  happy th birthday to my boo of alll time tupac...       1\n",
       "1596040                               happy charitytuesday       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 1564120 entries with 50.02% negative, 49.98% positive\n",
      "Validation set has total 15960 entries with 49.45% negative, 50.55% positive\n",
      "Test set has total 15961 entries with 49.68% negative, 50.32% positive\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x = df.text\n",
    "y = df.target\n",
    "SEED = 2000\n",
    "\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)\n",
    "print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train), (len(x_train[y_train == 0]) / (len(x_train)*1.))*100, (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation), (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100, (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n",
    "print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test), (len(x_test[y_test == 0]) / (len(x_test)*1.))*100, (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 0\n",
    "for x in x_train:\n",
    "    maxlen = max(maxlen, len(x.split()))\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_seq = pad_sequences(sequences, maxlen=45)\n",
    "sequences_val = tokenizer.texts_to_sequences(x_validation)\n",
    "x_val_seq = pad_sequences(sequences_val, maxlen=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288048                    your not pregnant oh no what shame\n",
       "357753                                 cleaning the bathroom\n",
       "420123     feeling left out you never recommend anything ...\n",
       "348643     home sick what the hell wonder if it ll mutate...\n",
       "1195630        your tweet reminded me that game was the shit\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,    39,     7,  3329,    75,    35,    46,   775],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,   696,     2,  2079],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          178,   225,    31,     6,   152,  2180,   319,     1,    14],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,    72,   171,    46,     2,\n",
       "          428,   719,    65,     4,    73, 45204,   229,  1287,   741],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           39,   217,  2968,    14,    13,   275,    23,     2,   354]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the embedding matrix constant\n",
    "\n",
    "model_ptw2v = Sequential()\n",
    "e = Embedding(100000, 200, weights=[embedding_matrix], input_length=45, trainable=False)\n",
    "model_ptw2v.add(e)\n",
    "model_ptw2v.add(Flatten())\n",
    "model_ptw2v.add(Dense(256, activation='relu'))\n",
    "model_ptw2v.add(Dense(1, activation='sigmoid'))\n",
    "model_ptw2v.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_ptw2v.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn the embedding matrix from scratch\n",
    "\n",
    "model_ptw2v = Sequential()\n",
    "e = Embedding(100000, 200, input_length=45)\n",
    "model_ptw2v.add(e)\n",
    "model_ptw2v.add(Flatten())\n",
    "model_ptw2v.add(Dense(256, activation='relu'))\n",
    "model_ptw2v.add(Dense(1, activation='sigmoid'))\n",
    "model_ptw2v.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_ptw2v.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the initial embedding matrix, but train it as well \n",
    "\n",
    "model_ptw2v = Sequential()\n",
    "e = Embedding(100000, 200, weights=[embedding_matrix], input_length=45, trainable=True)\n",
    "model_ptw2v.add(e)\n",
    "model_ptw2v.add(Flatten())\n",
    "model_ptw2v.add(Dense(256, activation='relu'))\n",
    "model_ptw2v.add(Dense(1, activation='sigmoid'))\n",
    "model_ptw2v.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_ptw2v.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 45, 200)           20000000  \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 44, 100)           40100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 20,040,100\n",
      "Trainable params: 20,040,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Basic model architecture of 1-D convolutional layer\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "structure_test = Sequential()\n",
    "e = Embedding(100000, 200, input_length=45)\n",
    "structure_test.add(e)\n",
    "structure_test.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "structure_test.add(GlobalMaxPooling1D())\n",
    "structure_test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static and non-trainable pre computed embedding matrix\n",
    "\n",
    "model_cnn_01 = Sequential()\n",
    "e = Embedding(100000, 200, weights=[embedding_matrix], input_length=45, trainable=False)\n",
    "model_cnn_01.add(e)\n",
    "model_cnn_01.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_01.add(GlobalMaxPooling1D())\n",
    "model_cnn_01.add(Dense(256, activation='relu'))\n",
    "model_cnn_01.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_01.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn_01.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn the embedding matrix from scratch \n",
    "\n",
    "model_cnn_02 = Sequential()\n",
    "e = Embedding(100000, 200, input_length=45)\n",
    "model_cnn_02.add(e)\n",
    "model_cnn_02.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_02.add(GlobalMaxPooling1D())\n",
    "model_cnn_02.add(Dense(256, activation='relu'))\n",
    "model_cnn_02.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_02.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn_02.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static and non-trainable pre computed embedding matrix, but train it as well\n",
    "\n",
    "model_cnn_02 = Sequential()\n",
    "e = Embedding(100000, 200, weights=[embedding_matrix], input_length=45, trainable=True)\n",
    "model_cnn_02.add(e)\n",
    "model_cnn_02.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_02.add(GlobalMaxPooling1D())\n",
    "model_cnn_02.add(Dense(256, activation='relu'))\n",
    "model_cnn_02.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_02.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn_02.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 45, 200)      20000000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 44, 100)      40100       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 43, 100)      60100       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 42, 100)      80100       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 100)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 100)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 100)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300)          0           global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            257         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 20,257,613\n",
      "Trainable params: 20,257,613\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "tweet_input = Input(shape=(45,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(100000, 200, weights=[embedding_matrix], input_length=45, trainable=True)(tweet_input)\n",
    "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"CNN_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "model.fit(x_train_seq, y_train, batch_size=32, epochs=5,\n",
    "                     validation_data=(x_val_seq, y_validation), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_CNN_model = load_model('CNN_best_weights.02-0.8333.hdf5')\n",
    "loaded_CNN_model.evaluate(x=x_val_seq, y=y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Evaluation with Test Set\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tvec = TfidfVectorizer(max_features=100000, ngram_range=(1, 3))\n",
    "x_train_tfidf = tvec.fit_transform(x_train)\n",
    "x_test_tfidf = tvec.transform(x_test)\n",
    "lr_with_tfidf = LogisticRegression()\n",
    "lr_with_tfidf.fit(x_train_tfidf, y_train)\n",
    "yhat_lr = lr_with_tfidf.predict_proba(x_test_tfidf)\n",
    "lr_with_tfidf.score(x_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_seq = pad_sequences(sequences_test, maxlen=45)\n",
    "yhat_cnn = loaded_CNN_model.predict(x_test_seq)\n",
    "loaded_CNN_model.evaluate(x=x_test_seq, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_test, yhat_lr[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "fpr_cnn, tpr_cnn, threshold = roc_curve(y_test, yhat_cnn)\n",
    "roc_auc_nn = auc(fpr_cnn, tpr_cnn)\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.plot(fpr, tpr, label='tfidf-logit (area = %0.3f)' % roc_auc, linewidth=2)\n",
    "plt.plot(fpr_cnn, tpr_cnn, label='w2v-CNN (area = %0.3f)' % roc_auc_nn, linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('Receiver operating characteristic: is positive', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
